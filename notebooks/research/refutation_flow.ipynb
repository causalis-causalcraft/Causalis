{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Refutation of DML IRM inference",
   "id": "d00547a1e10cbd13"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This notebook explain refutation tests that implemented in Causalis\n",
    "DGP took from [benchmarking notebook](https://causalis.causalcraft.com/research/dgp_benchmarking.html)"
   ],
   "id": "584cb05de29835c4"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-21T11:47:56.678474Z",
     "start_time": "2025-10-21T11:47:56.395946Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from causalis.data_contracts import CausalDatasetGenerator\n",
    "\n",
    "# 1) confounders\n",
    "confounder_specs: List[Dict[str, Any]] = [\n",
    "    {\"name\": \"tenure_months\",     \"dist\": \"normal\",    \"mu\": 24, \"sd\": 12},\n",
    "    {\"name\": \"avg_sessions_week\", \"dist\": \"normal\",    \"mu\": 5,  \"sd\": 2},\n",
    "    {\"name\": \"spend_last_month\",  \"dist\": \"uniform\",   \"a\": 0,   \"b\": 200},\n",
    "    {\"name\": \"premium_user\",      \"dist\": \"bernoulli\", \"p\": 0.25},\n",
    "    {\"name\": \"urban_resident\",    \"dist\": \"bernoulli\", \"p\": 0.60},\n",
    "]\n",
    "\n",
    "# 2) Feature index map\n",
    "def feature_indices_from_specs(specs: List[Dict[str, Any]]) -> Dict[str, Tuple[int, ...]]:\n",
    "    idx, out = 0, {}\n",
    "    for spec in specs:\n",
    "        name = spec.get(\"name\", \"\")\n",
    "        dist = str(spec.get(\"dist\",\"normal\")).lower()\n",
    "        if dist in (\"normal\",\"uniform\",\"bernoulli\"):\n",
    "            out[name] = (idx,); idx += 1\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dist: {dist}\")\n",
    "    return out\n",
    "\n",
    "feat = feature_indices_from_specs(confounder_specs)\n",
    "def col(X, key): return X[:, feat[key][0]]\n",
    "def _log1p_pos(x): return np.log1p(np.clip(x, 0.0, None))\n",
    "def _sqrt_pos(x):  return np.sqrt(np.clip(x, 0.0, None))\n",
    "def _ind(cond):    return cond.astype(float)\n",
    "def _sigmoid(z):   return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "# 3) g_d(x) -   nonlinear connection between X and D\n",
    "def g_d(X: np.ndarray) -> np.ndarray:\n",
    "    tenure = col(X, \"tenure_months\")\n",
    "    sess   = col(X, \"avg_sessions_week\")\n",
    "    spend  = col(X, \"spend_last_month\")\n",
    "    prem   = col(X, \"premium_user\")\n",
    "    urban  = col(X, \"urban_resident\")\n",
    "    tau_align = tau_func(X)                                       # explicit alignment\n",
    "    return (\n",
    "        1.10 * np.tanh(0.06*(spend - 100.0))\n",
    "      + 1.00 * _sigmoid(0.60*(sess - 5.0))\n",
    "      + 0.50 * _log1p_pos(tenure)\n",
    "      + 0.50 * prem\n",
    "      + 0.25 * urban\n",
    "      + 0.90 * prem * _ind(spend > 120.0)\n",
    "      + 0.30 * urban * _ind(tenure < 12.0)\n",
    "      + 0.80 * tau_align                                         # direct alignment term (λ)\n",
    "    )\n",
    "\n",
    "\n",
    "# 4) g_y(x)  -   nonlinear connection between X and Y\n",
    "def g_y(X: np.ndarray) -> np.ndarray:\n",
    "    tenure = col(X, \"tenure_months\")\n",
    "    sess   = col(X, \"avg_sessions_week\")\n",
    "    spend  = col(X, \"spend_last_month\")\n",
    "    prem   = col(X, \"premium_user\")\n",
    "    urban  = col(X, \"urban_resident\")\n",
    "    return (\n",
    "        0.70 * np.tanh(0.03*(spend - 80.0))\n",
    "      + 0.50 * _sqrt_pos(sess)\n",
    "      + 0.40 * _log1p_pos(tenure)\n",
    "      + 0.30 * prem\n",
    "      + 0.10 * urban\n",
    "      - 0.10 * _ind(spend < 20.0)\n",
    "    )\n",
    "\n",
    "# 5) tau(x)  — nonlinear effect function (CATE)\n",
    "def tau_func(X: np.ndarray) -> np.ndarray:\n",
    "    tenure = col(X, \"tenure_months\")\n",
    "    sess   = col(X, \"avg_sessions_week\")\n",
    "    spend  = col(X, \"spend_last_month\")\n",
    "    prem   = col(X, \"premium_user\")\n",
    "    urban  = col(X, \"urban_resident\")\n",
    "    return (\n",
    "        0.40\n",
    "      + 0.60 * (1.0 / (1.0 + np.exp(-0.40*(sess - 5.0))))  # sigmoid\n",
    "      + 2 * prem * _ind(spend > 120.0)\n",
    "      + 0.10 * urban * _ind(tenure < 12.0)\n",
    "    )\n",
    "\n",
    "# 6) Generator — continuous outcome\n",
    "gen = CausalDatasetGenerator(\n",
    "    theta=0.0,                 # ignored; we pass tau\n",
    "    tau=tau_func,              # nonlinear effect\n",
    "    beta_y=None, beta_d=None,  # use nonlinear g_* only\n",
    "    g_y=g_y, g_d=g_d,          # nonlinear functions\n",
    "    alpha_y=0.0,               # baseline mean level, intercept\n",
    "    alpha_d=0.0,               # will be calibrated to target_d_rate\n",
    "    sigma_y=1.0,               # noise std for Y\n",
    "    outcome_type=\"continuous\", # outcome distribution\n",
    "    confounder_specs=confounder_specs,\n",
    "    target_d_rate=0.20,        # 20% will be treated\n",
    "    u_strength_d=0.0,          # strength of latent confounder influence on treatment\n",
    "    u_strength_y=0.0,          # strength of latent confounder influence on outcome\n",
    "    propensity_sharpness=1,    # increase to make overlap harder\n",
    "    seed=123                   # random seed for reproducibility\n",
    ")\n",
    "\n",
    "# 7) Generate\n",
    "n = 10_000                     # Number of observations\n",
    "df = gen.generate(n)\n",
    "\n",
    "\n",
    "print(\"Treatment share ≈\", df[\"d\"].mean())\n",
    "true_ate = float(df[\"cate\"].mean())\n",
    "print(f\"Ground-truth ATE from the DGP: {true_ate:.3f}\")\n",
    "# Ground-truth ATT (on the natural scale): E[tau(X) | T=1] = mean CATE among the treated\n",
    "true_att = float(df.loc[df[\"d\"] == 1, \"cate\"].mean())\n",
    "print(f\"Ground-truth ATT from the DGP: {true_att:.3f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treatment share ≈ 0.2036\n",
      "Ground-truth ATE from the DGP: 0.913\n",
      "Ground-truth ATT from the DGP: 1.567\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:47:56.690462Z",
     "start_time": "2025-10-21T11:47:56.681717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from causalis.data_contracts import CausalData\n",
    "\n",
    "causal_data = CausalData(\n",
    "    df=df,\n",
    "    treatment=\"d\",\n",
    "    outcome=\"y\",\n",
    "    confounders=[\"tenure_months\",\n",
    "                 \"avg_sessions_week\",\n",
    "                 \"spend_last_month\",\n",
    "                 \"premium_user\",\n",
    "                 \"urban_resident\"]\n",
    ")\n",
    "\n",
    "causal_data.df.head()"
   ],
   "id": "4ca0a6271a4708ce",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          y    d  tenure_months  avg_sessions_week  spend_last_month  \\\n",
       "0  0.689404  0.0      12.130544           4.056687        181.570607   \n",
       "1  3.045282  0.0      19.586560           1.671561        182.793598   \n",
       "2  7.173595  1.0      39.455103           5.452889        125.185708   \n",
       "3  1.926216  0.0      26.327693           5.051629          4.932905   \n",
       "4  1.225088  0.0      35.042771           4.933996         23.577407   \n",
       "\n",
       "   premium_user  urban_resident  \n",
       "0           0.0             0.0  \n",
       "1           0.0             0.0  \n",
       "2           1.0             1.0  \n",
       "3           0.0             1.0  \n",
       "4           0.0             0.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>d</th>\n",
       "      <th>tenure_months</th>\n",
       "      <th>avg_sessions_week</th>\n",
       "      <th>spend_last_month</th>\n",
       "      <th>premium_user</th>\n",
       "      <th>urban_resident</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.689404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.130544</td>\n",
       "      <td>4.056687</td>\n",
       "      <td>181.570607</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.045282</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.586560</td>\n",
       "      <td>1.671561</td>\n",
       "      <td>182.793598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.173595</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.455103</td>\n",
       "      <td>5.452889</td>\n",
       "      <td>125.185708</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.926216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.327693</td>\n",
       "      <td>5.051629</td>\n",
       "      <td>4.932905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.225088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.042771</td>\n",
       "      <td>4.933996</td>\n",
       "      <td>23.577407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "3c707fbd661ac9d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:48:05.559263Z",
     "start_time": "2025-10-21T11:47:56.727054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from causalis.scenarios.unconfoundedness.ate import dml_ate\n",
    "\n",
    "# Estimate Average Treatment Effect (ATE)\n",
    "ate_result = dml_ate(causal_data, n_folds=4, normalize_ipw=False, store_diagnostic_data=True, random_state=123)"
   ],
   "id": "17518eeb42fdb668",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:48:05.591600Z",
     "start_time": "2025-10-21T11:48:05.589676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(ate_result.get('coefficient'))\n",
    "print(ate_result.get('p_value'))\n",
    "print(ate_result.get('confidence_interval'))\n",
    "print(f\"Ground-truth ATE from the DGP: {true_ate:.3f}\")"
   ],
   "id": "5038847ae81d6485",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9917276396749556\n",
      "0.0\n",
      "(0.869543879249174, 1.1139114001007373)\n",
      "Ground-truth ATE from the DGP: 0.913\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we see our estimate is accurate and CI bounds include ground-truth ATE. In real life we can't compare estimation with truth so we need\n",
    "check robustness of it: run some tests on assumptions and answer questions about research design"
   ],
   "id": "6d7fab8a4d9a287d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Overlap",
   "id": "83000ad88108b198"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## What “overlap/positivity” means",
   "id": "94e94eff03e44bd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Binary treatment $T \\in \\{0,1\\}$:**\n",
    "for all confounder values $x$ in your target population,\n",
    "\n",
    "$$\n",
    "0 < e(x) := P(T = 1 \\mid X = x) < 1,\n",
    "$$\n",
    "\n",
    "often strengthened to **strong positivity**:\n",
    "there exists an $\\varepsilon > 0$ such that\n",
    "\n",
    "$$\n",
    "\\varepsilon \\le e(x) \\le 1 - \\varepsilon\n",
    "\\quad\\text{almost surely.}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Why it matters\n",
    "\n",
    "* **Identification:**\n",
    "  Overlap + uncofoundedness are the two pillars that identify causal effects from observational data.\n",
    "  Without overlap, the effect is **not identified** — you must extrapolate or model-specify what never occurs.\n",
    "\n",
    "* **Estimation stability:**\n",
    "  IPW/DR estimators use weights\n",
    "\n",
    "  $$\n",
    "  w_1 = \\frac{D}{e(X)}, \\qquad\n",
    "  w_0 = \\frac{1 - D}{1 - e(X)}.\n",
    "  $$\n",
    "\n",
    "  If $e(X)$ is near 0 or 1, these weights explode, causing huge variance and fragile estimates.\n",
    "\n",
    "* **Target population:**\n",
    "  With trimming or restriction, you may change *who* the effect describes —\n",
    "  e.g., ATE on the **region of common support**, not on the full population.\n"
   ],
   "id": "7a6b0cd9e6cd38bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It's summary for overlap diagnostics",
   "id": "5d14271c97426c51"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:48:05.612797Z",
     "start_time": "2025-10-21T11:48:05.598513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from causalis.scenarios.unconfoundedness.refutation import *\n",
    "rep = run_overlap_diagnostics(res=ate_result)\n",
    "rep[\"summary\"]"
   ],
   "id": "9b50d93eee1acd33",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                 metric      value    flag\n",
       "0       edge_0.01_below   0.000000   GREEN\n",
       "1       edge_0.01_above   0.000000   GREEN\n",
       "2       edge_0.02_below   0.077300  YELLOW\n",
       "3       edge_0.02_above   0.000400  YELLOW\n",
       "4                    KS   0.511643     RED\n",
       "5                   AUC   0.835125  YELLOW\n",
       "6     ESS_treated_ratio   0.247034  YELLOW\n",
       "7     ESS_control_ratio   0.327069   GREEN\n",
       "8      tails_w1_q99/med  38.676284  YELLOW\n",
       "9      tails_w0_q99/med  20.575638  YELLOW\n",
       "10  ATT_identity_relerr   0.177229     RED\n",
       "11         clip_m_total   0.023600  YELLOW\n",
       "12            calib_ECE   0.018453   GREEN\n",
       "13          calib_slope   0.889332   GREEN\n",
       "14      calib_intercept  -0.106806   GREEN"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>edge_0.01_below</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>GREEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>edge_0.01_above</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>GREEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>edge_0.02_below</td>\n",
       "      <td>0.077300</td>\n",
       "      <td>YELLOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>edge_0.02_above</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>YELLOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KS</td>\n",
       "      <td>0.511643</td>\n",
       "      <td>RED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AUC</td>\n",
       "      <td>0.835125</td>\n",
       "      <td>YELLOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ESS_treated_ratio</td>\n",
       "      <td>0.247034</td>\n",
       "      <td>YELLOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ESS_control_ratio</td>\n",
       "      <td>0.327069</td>\n",
       "      <td>GREEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tails_w1_q99/med</td>\n",
       "      <td>38.676284</td>\n",
       "      <td>YELLOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tails_w0_q99/med</td>\n",
       "      <td>20.575638</td>\n",
       "      <td>YELLOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ATT_identity_relerr</td>\n",
       "      <td>0.177229</td>\n",
       "      <td>RED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>clip_m_total</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>YELLOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>calib_ECE</td>\n",
       "      <td>0.018453</td>\n",
       "      <td>GREEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>calib_slope</td>\n",
       "      <td>0.889332</td>\n",
       "      <td>GREEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>calib_intercept</td>\n",
       "      <td>-0.106806</td>\n",
       "      <td>GREEN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## `edge_mass`",
   "id": "7e6f2c464fb49a60"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`edge_0.01_below`, `edge_0.01_above`, `edge_0.02_below`, `edge_0.02_above`  are shares of units whose propensity is below or above the percents\n",
    "\n",
    "To keep in mind: DML IRM is clipping out the interval [0.02 and 0.98]\n",
    "\n",
    "Huge shares are dangerous for estimation in terms of weights exploding"
   ],
   "id": "acf00095707e992f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Flags in Causalis:\n",
    "\n",
    "For $ε=0.01$: YELLOW if either side 0.02 (2%), RED if 0.05 (5%).\n",
    "\n",
    "For $ε=0.02$: YELLOW if either side 0.05 (5%), RED if 0.10 (10%)."
   ],
   "id": "3eeb0b47996e46cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:48:05.640216Z",
     "start_time": "2025-10-21T11:48:05.637995Z"
    }
   },
   "cell_type": "code",
   "source": "rep['edge_mass']",
   "id": "f092f85fa1ab5e45",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'share_below_001': 0.0,\n",
       " 'share_above_001': 0.0,\n",
       " 'share_below_002': 0.0773,\n",
       " 'share_above_002': 0.0004,\n",
       " 'min_m': 0.01,\n",
       " 'max_m': 0.99}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:48:05.670529Z",
     "start_time": "2025-10-21T11:48:05.668398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# You can also check shares per arm\n",
    "rep['edge_mass_by_arm']"
   ],
   "id": "5ca41dba0b82b958",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'share_below_001_D1': 0.0,\n",
       " 'share_above_001_D0': 0.0,\n",
       " 'share_below_002_D1': 0.010805500982318271,\n",
       " 'share_above_002_D0': 0.00025113008538422905}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## `ks - Kolmogorov–Smirnov statistic`",
   "id": "64fa3f42adce7728"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here KS is the **two-sample Kolmogorov–Smirnov statistic** comparing the distributions of the propensities for treated vs control:\n",
    "\n",
    "$$ D=\\max_t |\\hat F_A(t)-\\hat F_B(t)| $$\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "* (D=0): identical distributions (perfect overlap).\n",
    "* (D=1): complete separation (no overlap).\n",
    "* Your value **KS = 0.5116** means there exists a threshold (t) such that the share of treated with $(m\\le t)$ differs from the share of controls with $(m\\le t)$ by **~51 percentage points**. That’s why it’s flagged **RED** (your thresholds mark RED when (D>0.35)): treatment assignment is highly predictable from covariates ⇒ **poor overlap / strong cofounding risk**.\n"
   ],
   "id": "29c05124aa1b8f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:48:05.681827Z",
     "start_time": "2025-10-21T11:48:05.679944Z"
    }
   },
   "cell_type": "code",
   "source": "rep['ks']",
   "id": "61fd3b081e096246",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5116427657267132"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## `AUC`",
   "id": "ad9b4b6728415ea1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Probability definition (most intuitive)\n",
    "\n",
    "$$\n",
    "\\text{AUC} = \\Pr\\big(s^+ > s^-\\big)+\\tfrac12,\\Pr\\big(s^+ = s^-\\big),\n",
    "$$\n",
    "where $(s^+)$ is a score from a random positive and $(s^-)$ from a random negative.\n",
    "So AUC is the fraction of all $(n_1 n_0)$ positive–negative pairs that are correctly ordered by the score (ties get half-credit).\n",
    "\n",
    "### Rank / Mann–Whitney formulation\n",
    "\n",
    "1. Rank all scores together (ascending). If there are ties, assign **average ranks** within each tied block.\n",
    "2. Let $(R_1)$ be the **sum of ranks** for the positives.\n",
    "3. Compute the Mann–Whitney (U) statistic for positives:\n",
    "\n",
    "   $$\n",
    "   U_1 = R_1 - \\frac{n_1(n_1+1)}{2}.\n",
    "   $$\n",
    "\n",
    "4. Convert to AUC by normalizing:\n",
    "\n",
    "   $$\n",
    "   \\boxed{\\text{AUC} = \\frac{U_1}{n_1 n_0}\n",
    "   = \\frac{R_1 - \\frac{n_1(n_1+1)}{2}}{n_1 n_0}}\n",
    "   $$\n",
    "\n",
    "   This is exactly what your function returns (with stable sorting and tie-averaged ranks).\n",
    "\n",
    "### ROC-integral view (equivalent)\n",
    "\n",
    "If $(\\text{TPR}(t))$ and $(\\text{FPR}(t))$ trace the ROC curve as the threshold $(t)$ moves,\n",
    "\n",
    "$$\n",
    "\\text{AUC} = \\int_0^1 \\text{TPR}\\big(\\text{FPR}^{-1}(u)\\big)du,\n",
    "$$\n",
    "\n",
    "i.e., the geometric area under the ROC.\n",
    "\n",
    "### Properties you should remember\n",
    "\n",
    "* **Range:** $(0 \\le \\text{AUC} \\le 1)$; 0.5 = random ranking; 1 = perfect separation.\n",
    "* **Symmetry:** $(\\text{AUC}(s,y) = 1 - \\text{AUC}(s,1-y))$.\n",
    "* **Monotone invariance:** Any strictly increasing transform $(f)$ leaves AUC unchanged (only ranks matter).\n",
    "* **Ties:** Averaged ranks ⇒ adds the $(\\tfrac12\\Pr(s^+=s^-))$ term automatically.\n",
    "\n",
    "### In the propensity/overlap context\n",
    "\n",
    "* A **higher** AUC means treatment (D) is **more predictable** from covariates (bad for overlap/positivity).\n",
    "* For good overlap you actually want AUC **close to 0.5**.\n"
   ],
   "id": "7b40ba8a683865c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:48:05.694592Z",
     "start_time": "2025-10-21T11:48:05.692447Z"
    }
   },
   "cell_type": "code",
   "source": "rep['auc']",
   "id": "8f396c3b720a8608",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8351248965136829"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## `ESS_treated_ratio`",
   "id": "692fbcc1db4e5f1f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Weights used\n",
    "\n",
    "For ATE-style IPW, the treated-arm weights are\n",
    "\n",
    "$$\n",
    "w_i ;=; \\frac{D_i}{m_i}\n",
    "\\quad\\Rightarrow\\quad\n",
    "w_i =\n",
    "\\begin{cases}\n",
    "1/m_i,& D_i=1\\\n",
    "when 0, & D_i=0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "so on the treated subset $({i:D_i=1})$ the weights are simply $(1/m_i)$.\n",
    "\n",
    "### Effective sample size (ESS)\n",
    "\n",
    "Given the treated-arm weights $(w_1,\\ldots,w_{n_1})$ (only for $(D=1)$),\n",
    "\n",
    "$$\n",
    "\\mathrm{ESS} = \\frac{\\left(\\sum_{i=1}^{n_1} w_i\\right)^2}{\\sum_{i=1}^{n_1} w_i^2}.\n",
    "$$\n",
    "\n",
    "This is exactly what `_ess(w)` computes.\n",
    "\n",
    "* If all treated weights are equal, ESS $(= n_1)$ (full efficiency).\n",
    "* If a few weights dominate, ESS drops (information concentrated in few units).\n",
    "\n",
    "### The reported metric\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\mathrm{ESS}_{\\text{treated ratio}}\n",
    "= \\frac{\\mathrm{ESS}}{n_1}\n",
    "= \\frac{\\left(\\sum_i w_i\\right)^2}{n_1 \\sum_i w_i^2}\n",
    "}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "This lies in $(0,1]$. Near **1** ⇒ well-behaved weights; near **0** ⇒ severe instability.\n",
    "\n",
    "### Why it reflects overlap\n",
    "\n",
    "When propensities $(m_i)$ approach 0 for treated units, weights $(1/m_i)$ explode → large CV → **low ESS_treated_ratio**. Hence this metric is a direct, quantitative read on how much usable information remains in the treated group after IPW.\n"
   ],
   "id": "80cc9314e4fc0d41"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:48:05.702520Z",
     "start_time": "2025-10-21T11:48:05.701067Z"
    }
   },
   "cell_type": "code",
   "source": "print(rep['ate_ess'])",
   "id": "244896913a18cc7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ess_w1': 502.960611352384, 'ess_w0': 2604.778026253534, 'ess_ratio_w1': 0.2470336990925265, 'ess_ratio_w0': 0.32706906407000674}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## `tails_w1_q99/med`",
   "id": "f00a471d1d90d724"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\boxed{\n",
    "\\texttt{tails\\_w1\\_q99/med}\n",
    "= \\frac{Q_{0.99}(W_1)}{\\mathrm{median}(W_1)}.\n",
    "}\n",
    "$$\n",
    "\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "* It’s a **tail-heaviness index** for treated weights: how large the 99th-percentile weight is relative to a typical (median) weight.\n",
    "* **Scale-invariant**: if you re-scale weights (e.g., Hájek normalization), both numerator and denominator scale equally, so the ratio is unchanged.\n",
    "* Bigger $(\\Rightarrow)$ **heavier right tail** $(\\Rightarrow)$ more variance inflation for IPW (since variance depends on large $(w_i^2)$). It typically coincides with a **low $ESS(_\\text{treated ratio}$)**.\n",
    "\n",
    "### Edge cases & thresholds\n",
    "\n",
    "* If $(\\text{median}(W_1)=0)$ or undefined, the ratio is not meaningful (your code returns “NA” in that case; with positive treated weights this is rare).\n",
    "* Defaults: **YELLOW** if any of $({q95/med,q99/med,q999/med,\\max/med})$ exceeds **10**; **RED** if any exceed **100**.\n",
    "  `tails_w1_q99/med` is one of these checks, focusing specifically on the 99th percentile.\n",
    "\n",
    "### Quick example\n",
    "\n",
    "If $\\mathrm{median}(W_1) = 1.2$ and $Q_{0.99}(W_1) = 46.8$, then\n",
    "\n",
    "$$\n",
    "\\texttt{tails\\_w1\\_q99/med}\n",
    "= \\frac{46.8}{1.2} \\approx 39,\n",
    "$$\n",
    "\n",
    "indicating heavy tails and a likely unstable ATE IPW.\n",
    "\n"
   ],
   "id": "7b074d101d913f4e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:48:05.714105Z",
     "start_time": "2025-10-21T11:48:05.712474Z"
    }
   },
   "cell_type": "code",
   "source": "print(rep['ate_tails'])",
   "id": "356abbf0eab1cf25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w1': {'q50': 2.585563809098619, 'q95': 26.04879283279811, 'q99': 100.0, 'max': 100.0, 'median': 2.585563809098619}, 'w0': {'q50': 1.073397908573178, 'q95': 1.6626662464619888, 'q99': 22.085846285748335, 'max': 99.99999999999991, 'median': 1.073397908573178}}\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## `ATT_identity_relerr`",
   "id": "119167fffefdaf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "With estimated propensities $(m_i=\\hat m(X_i))$ and $(D_i\\in{0,1})$:\n",
    "\n",
    "* **Left-hand side (controls odds sum):**\n",
    "  $$\n",
    "  \\text{LHS} = \\sum_{i=1}^n (1-D_i),\\frac{m_i}{1-m_i}.\n",
    "  $$\n",
    "* **Right-hand side (treated count):**\n",
    "  $$\n",
    "  \\text{RHS} = \\sum_{i=1}^n D_i = n_1.\n",
    "  $$\n",
    "\n",
    "If $(\\hat m\\approx m)$ and overlap is ok, **LHS $(\\approx)$ RHS**.\n",
    "\n",
    "You report the **relative error**:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\texttt{ATT\\_identity\\_relerr}\n",
    "= \\frac{\\big|\\mathrm{LHS} - \\mathrm{RHS}\\big|}{\\mathrm{RHS}}\n",
    "}\n",
    "$$\n",
    "\n",
    "(when $(n_1>0)$ otherwise it’s set to $(\\infty)$).\n",
    "\n",
    "\n",
    "### How to read the number\n",
    "\n",
    "* Small $(\\texttt{relerr})$ (e.g., (\\le 5%)) ⇒ propensities are **reasonably calibrated** (especially on the control side) and ATT weights won’t be wildly off in total mass.\n",
    "* Large $(\\texttt{relerr})$ ⇒ possible **miscalibration** of $(\\hat m)$ (e.g., over/underestimation for controls), **poor overlap** (many controls with $(m_i\\to 1)$ inflating $(m_i/(1-m_i)))$, or **clipping/trimming** effects.\n",
    "\n",
    "Your default flags (same as in the code):\n",
    "\n",
    "* **GREEN** if $(\\texttt{relerr} \\le 0.05)$\n",
    "* **YELLOW** if $(0.05 < \\texttt{relerr} \\le 0.10)$\n",
    "* **RED** if $(> 0.10)$\n",
    "\n",
    "### Quick intuition\n",
    "\n",
    "The term $(m/(1-m))$ is the **odds** of treatment. Summing that over **controls** should reconstruct the **treated count**. If it doesn’t, either the odds are off (propensity miscalibration) or the data lack support where you need it—both are red flags for ATT-IPW stability.\n"
   ],
   "id": "2d71e26bd2078726"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:48:05.733630Z",
     "start_time": "2025-10-21T11:48:05.731938Z"
    }
   },
   "cell_type": "code",
   "source": "print(rep['att_weights'])",
   "id": "a5333071edff46f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lhs_sum': 2396.83831663893, 'rhs_sum': 2036.0, 'rel_err': 0.17722903567727402}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## `clip_m_total`",
   "id": "58dc187066c889df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "look at edge_mass",
   "id": "4e27b5eeb63aa2b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:48:05.742963Z",
     "start_time": "2025-10-21T11:48:05.741510Z"
    }
   },
   "cell_type": "code",
   "source": "print(rep['clipping'])",
   "id": "68d10c2c4d0000fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'m_clip_lower': 0.0235, 'm_clip_upper': 0.0001, 'g_clip_share': nan}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## `calib_ECE, calib_slope, calib_intercept`",
   "id": "492ba9092658cbd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### calib_ECE = 0.018 (GREEN)\n",
    "\n",
    "**Math:** with 10 equal-width bins,\n",
    "\n",
    "$$\n",
    "\\text{ECE}=\\sum_{k=1}^{10}\\frac{n_k}{n},|\\bar y_k-\\bar p_k|\n",
    "$$\n",
    "\n",
    "(weighted average gap between observed rate (\\bar y_k) and mean prediction (\\bar p_k) per bin).\n",
    "**Result:** ~1.8% average miscalibration → overall probabilities track outcomes well. Note the biggest bin error is in 0.5–0.6 (abs_error ≈ 0.162) but it’s tiny (95/10,000), so ECE stays low.\n",
    "\n",
    "### calib_slope (β) = 0.889 (GREEN)\n",
    "\n",
    "**Math (logistic recalibration):**\n",
    "\n",
    "$$\n",
    "\\Pr(D=1\\mid p)=\\sigma(\\alpha+\\beta,\\text{logit}(p)).\n",
    "$$\n",
    "\n",
    "**Interpretation:** (\\beta<1) ⇒ predictions are a bit **over-confident** (too extreme); the optimal calibration slightly **flattens** them toward 0.5.\n",
    "\n",
    "### calib_intercept (α) = −0.107 (GREEN)\n",
    "\n",
    "**Math:** same model as above; (\\alpha) is a vertical shift on the log-odds scale.\n",
    "**Interpretation:** Negative (\\alpha) nudges probabilities **downward** overall (your model is, on average, a bit high), consistent with bins like 0.5–0.6 where $(\\bar p_k > \\bar y_k)$.\n",
    "\n",
    "All three fall well within your GREEN thresholds, so calibration looks solid despite minor mid-range overprediction.\n"
   ],
   "id": "7aed96d93c0906a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:48:05.752126Z",
     "start_time": "2025-10-21T11:48:05.749653Z"
    }
   },
   "cell_type": "code",
   "source": "print(rep['calibration'])",
   "id": "17e1b91c64cefb02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 10000, 'n_bins': 10, 'auc': 0.8351248965136829, 'brier': 0.10778483728183921, 'ece': 0.018452696253043466, 'reliability_table':    bin  lower  upper  count    mean_p  frac_pos  abs_error\n",
      "0    0    0.0    0.1   5089  0.044279  0.054431   0.010152\n",
      "1    1    0.1    0.2   1724  0.148308  0.172274   0.023965\n",
      "2    2    0.2    0.3   1171  0.245443  0.231426   0.014017\n",
      "3    3    0.3    0.4    626  0.341419  0.316294   0.025125\n",
      "4    4    0.4    0.5    251  0.440675  0.382470   0.058205\n",
      "5    5    0.5    0.6     95  0.541182  0.378947   0.162235\n",
      "6    6    0.6    0.7    107  0.649163  0.588785   0.060378\n",
      "7    7    0.7    0.8    214  0.754985  0.785047   0.030061\n",
      "8    8    0.8    0.9    427  0.851461  0.859485   0.008024\n",
      "9    9    0.9    1.0    296  0.932652  0.888514   0.044139, 'recalibration': {'intercept': -0.10680601474031537, 'slope': 0.8893319945661962}, 'flags': {'ece': 'GREEN', 'slope': 'GREEN', 'intercept': 'GREEN'}, 'thresholds': {'ece_warn': 0.1, 'ece_strong': 0.2, 'slope_warn_lo': 0.8, 'slope_warn_hi': 1.2, 'slope_strong_lo': 0.6, 'slope_strong_hi': 1.4, 'intercept_warn': 0.2, 'intercept_strong': 0.4}}\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:48:05.765599Z",
     "start_time": "2025-10-21T11:48:05.761254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "calib = rep['calibration']\n",
    "\n",
    "calib['reliability_table']"
   ],
   "id": "dcf55bde5fe7d41d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   bin  lower  upper  count    mean_p  frac_pos  abs_error\n",
       "0    0    0.0    0.1   5089  0.044279  0.054431   0.010152\n",
       "1    1    0.1    0.2   1724  0.148308  0.172274   0.023965\n",
       "2    2    0.2    0.3   1171  0.245443  0.231426   0.014017\n",
       "3    3    0.3    0.4    626  0.341419  0.316294   0.025125\n",
       "4    4    0.4    0.5    251  0.440675  0.382470   0.058205\n",
       "5    5    0.5    0.6     95  0.541182  0.378947   0.162235\n",
       "6    6    0.6    0.7    107  0.649163  0.588785   0.060378\n",
       "7    7    0.7    0.8    214  0.754985  0.785047   0.030061\n",
       "8    8    0.8    0.9    427  0.851461  0.859485   0.008024\n",
       "9    9    0.9    1.0    296  0.932652  0.888514   0.044139"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>count</th>\n",
       "      <th>mean_p</th>\n",
       "      <th>frac_pos</th>\n",
       "      <th>abs_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5089</td>\n",
       "      <td>0.044279</td>\n",
       "      <td>0.054431</td>\n",
       "      <td>0.010152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1724</td>\n",
       "      <td>0.148308</td>\n",
       "      <td>0.172274</td>\n",
       "      <td>0.023965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1171</td>\n",
       "      <td>0.245443</td>\n",
       "      <td>0.231426</td>\n",
       "      <td>0.014017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>626</td>\n",
       "      <td>0.341419</td>\n",
       "      <td>0.316294</td>\n",
       "      <td>0.025125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>251</td>\n",
       "      <td>0.440675</td>\n",
       "      <td>0.382470</td>\n",
       "      <td>0.058205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>95</td>\n",
       "      <td>0.541182</td>\n",
       "      <td>0.378947</td>\n",
       "      <td>0.162235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>107</td>\n",
       "      <td>0.649163</td>\n",
       "      <td>0.588785</td>\n",
       "      <td>0.060378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>214</td>\n",
       "      <td>0.754985</td>\n",
       "      <td>0.785047</td>\n",
       "      <td>0.030061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>427</td>\n",
       "      <td>0.851461</td>\n",
       "      <td>0.859485</td>\n",
       "      <td>0.008024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296</td>\n",
       "      <td>0.932652</td>\n",
       "      <td>0.888514</td>\n",
       "      <td>0.044139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Score",
   "id": "11d103024345e11"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We need this score refutation tests for:\n",
    "* Catch overfitting/leakage: The out-of-sample moment check verifies that the AIPW score averages to ~0 on held-out folds using fold-specific θ and nuisances. If this fails, your effect can be an artifact of leakage or overfit learners rather than a real signal.\n",
    "* Verify Neyman orthogonality in practice: The Gateaux-derivative tests (orthogonality_derivatives) check that small, targeted perturbations to the nuisances (g₀, g₁, m) don’t move the score mean. Large |t| values flag miscalibration (e.g., biased propensity or outcome models) that breaks the orthogonality protection DML relies on.\n",
    "* Assess finite-sample stability: The influence diagnostics reveal heavy tails (p99/median, kurtosis) and top-influential points. Spiky ψ implies high variance and sensitivity—often due to near-0/1 propensities, poor overlap, or outliers.\n",
    "* ATTE-specific risks: For ATT/ATTE, only g₀ and m matter in the score. The added overlap metrics and trim curves show how reliant your estimate is on scarce, high-m controls—common failure mode for ATT."
   ],
   "id": "ab562b5f28a37f8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:48:05.795945Z",
     "start_time": "2025-10-21T11:48:05.786276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from causalis.scenarios.unconfoundedness.refutation.score.score_validation import run_score_diagnostics\n",
    "rep_score = run_score_diagnostics(res=ate_result)\n",
    "rep_score[\"summary\"]"
   ],
   "id": "521ef2522ba761d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             metric         value    flag\n",
       "0         se_plugin  6.233980e-02      NA\n",
       "1  psi_p99_over_med  2.374779e+01     RED\n",
       "2      psi_kurtosis  3.032000e+02     RED\n",
       "3        max_|t|_g1  4.350018e+00     RED\n",
       "4        max_|t|_g0  2.076780e+00  YELLOW\n",
       "5         max_|t|_m  1.030583e+00   GREEN\n",
       "6    oos_tstat_fold -2.552943e-15   GREEN\n",
       "7  oos_tstat_strict -2.461798e-15   GREEN"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>se_plugin</td>\n",
       "      <td>6.233980e-02</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>psi_p99_over_med</td>\n",
       "      <td>2.374779e+01</td>\n",
       "      <td>RED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>psi_kurtosis</td>\n",
       "      <td>3.032000e+02</td>\n",
       "      <td>RED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>max_|t|_g1</td>\n",
       "      <td>4.350018e+00</td>\n",
       "      <td>RED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max_|t|_g0</td>\n",
       "      <td>2.076780e+00</td>\n",
       "      <td>YELLOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max_|t|_m</td>\n",
       "      <td>1.030583e+00</td>\n",
       "      <td>GREEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>oos_tstat_fold</td>\n",
       "      <td>-2.552943e-15</td>\n",
       "      <td>GREEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>oos_tstat_strict</td>\n",
       "      <td>-2.461798e-15</td>\n",
       "      <td>GREEN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## `psi_p99_over_med`",
   "id": "ac23ea9b9ab50a30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Let $\\psi_i$ be the per-unit **influence value** (EIF score) for your estimator.\n",
    "  We look at magnitudes $a_i \\equiv |\\psi_i|$.\n",
    "\n",
    "* Define the 99th percentile and the median of these magnitudes:\n",
    "\n",
    "  $$\n",
    "  q_{0.99} \\equiv \\operatorname{Quantile}_{0.99}(a_1,\\dots,a_n),\\qquad\n",
    "  m \\equiv \\operatorname{median}(a_1,\\dots,a_n).\n",
    "  $$\n",
    "\n",
    "* The metric is the **scale-free tail ratio**:\n",
    "\n",
    "  $$\n",
    "  \\boxed{\n",
    "  \\texttt{psi\\_p99\\_over\\_med}\n",
    "  = \\frac{q_{0.99}}{m}\n",
    "  }\n",
    "  $$\n",
    "\n",
    "\n",
    "**Why this works (brief):**\n",
    "\n",
    "* Uses $|\\psi_i|$ to ignore sign (only tail size matters).\n",
    "* Dividing by the **median** makes it scale-invariant and robust to a few large values.\n",
    "* Large values $\\big(\\gg 1\\big)$ mean a small fraction of observations dominate uncertainty (heavy tails → unstable SE).\n",
    "\n",
    "**Quick read:**\n",
    "\n",
    "* $\\approx 1!-!5$: tails tame/stable\n",
    "* $\\gtrsim 10$: caution (heavy tails)\n",
    "* $\\gtrsim 20$: likely unstable; check overlap, trim/clamp propensities, or robustify learners.\n"
   ],
   "id": "38bd5c42c9f1f364"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:48:05.904749Z",
     "start_time": "2025-10-21T11:48:05.901612Z"
    }
   },
   "cell_type": "code",
   "source": "rep_score['influence_diagnostics']",
   "id": "6b9e5a43aafe636b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'se_plugin': 0.06233979878689177,\n",
       " 'kurtosis': 303.1999961346597,\n",
       " 'p99_over_med': 23.747788009323845,\n",
       " 'top_influential':       i         psi         m     res_t  res_c\n",
       " 0  6224  205.671733  0.010000  2.062391    0.0\n",
       " 1  1915 -180.280657  0.012198 -2.205898   -0.0\n",
       " 2   215 -163.974979  0.013644 -2.221508   -0.0\n",
       " 3  9389  131.757805  0.010585  1.393678    0.0\n",
       " 4   868 -101.111026  0.014727 -1.489752   -0.0\n",
       " 5  2741  -96.602896  0.024285 -2.345406   -0.0\n",
       " 6  1993   83.941140  0.028412  2.404237    0.0\n",
       " 7  9894  -82.585292  0.011016 -0.907962   -0.0\n",
       " 8  2350   70.269293  0.029162  2.063988    0.0\n",
       " 9  1499   70.103947  0.022092  1.576351    0.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## `psi_kurtosis`",
   "id": "d2e20b1f69e6efc4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Let $\\psi_i$ be the per-unit influence values and define centered residuals\n",
    "\n",
    "  $$\n",
    "  \\tilde\\psi_i \\equiv \\psi_i - \\bar\\psi,\\qquad\n",
    "  \\bar\\psi \\equiv \\frac{1}{n}\\sum_{i=1}^n \\psi_i.\n",
    "  $$\n",
    "\n",
    "* Sample variance (with Bessel correction):\n",
    "\n",
    "  $$\n",
    "  s^2 \\equiv \\frac{1}{n-1}\\sum_{i=1}^n \\tilde\\psi_i^2.\n",
    "  $$\n",
    "\n",
    "* Sample 4th central moment:\n",
    "\n",
    "  $$\n",
    "  \\hat\\mu_4 \\equiv \\frac{1}{n}\\sum_{i=1}^n \\tilde\\psi_i^4.\n",
    "  $$\n",
    "\n",
    "* The reported metric (raw kurtosis, **not** excess):\n",
    "\n",
    "  $$\n",
    "  \\boxed{\n",
    "  \\texttt{psi\\_kurtosis}\n",
    "  = \\frac{\\hat{\\mu}_4}{s^4}\n",
    "  }\n",
    "  $$\n",
    "\n",
    "\n",
    "**Interpretation (quick):**\n",
    "\n",
    "* Normal reference $\\approx 3$ (excess kurtosis $=0$).\n",
    "* Much larger $\\Rightarrow$ heavier tails / more extreme $\\psi_i$ outliers.\n",
    "* Rules of thumb used in the diagnostics: $\\ge 10$ = caution, $\\ge 30$ = severe.\n"
   ],
   "id": "1cd0df9f1fb38694"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:48:05.916292Z",
     "start_time": "2025-10-21T11:48:05.913575Z"
    }
   },
   "cell_type": "code",
   "source": "rep_score['influence_diagnostics']",
   "id": "f0f2937fe72b29e5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'se_plugin': 0.06233979878689177,\n",
       " 'kurtosis': 303.1999961346597,\n",
       " 'p99_over_med': 23.747788009323845,\n",
       " 'top_influential':       i         psi         m     res_t  res_c\n",
       " 0  6224  205.671733  0.010000  2.062391    0.0\n",
       " 1  1915 -180.280657  0.012198 -2.205898   -0.0\n",
       " 2   215 -163.974979  0.013644 -2.221508   -0.0\n",
       " 3  9389  131.757805  0.010585  1.393678    0.0\n",
       " 4   868 -101.111026  0.014727 -1.489752   -0.0\n",
       " 5  2741  -96.602896  0.024285 -2.345406   -0.0\n",
       " 6  1993   83.941140  0.028412  2.404237    0.0\n",
       " 7  9894  -82.585292  0.011016 -0.907962   -0.0\n",
       " 8  2350   70.269293  0.029162  2.063988    0.0\n",
       " 9  1499   70.103947  0.022092  1.576351    0.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### $max_|t|_g1$, $max_|t|_g0$, $max_|t|_m$",
   "id": "f69d2f0ff91e74f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We work with a **basis** of functions\n",
    "\n",
    "$$\n",
    "\\{h_b(X)\\}_{b=0}^{B-1}\n",
    "\\quad\\text{(columns of }X_{\\text{basis}}\\text{; }h_0\\equiv 1\\text{ is the constant).}\n",
    "$$\n",
    "\n",
    "Let $(m_i^\\tau \\equiv \\mathrm{clip}(m_i,\\tau,1-\\tau))$ be the clipped propensity (guards against division by zero).\n",
    "\n",
    "### ATE case\n",
    "\n",
    "For each basis function $(b)$, form a sample mean (Gateaux derivative estimator) and its standard error, then compute a t-statistic; finally take the maximum absolute value across bases.\n",
    "\n",
    "---\n",
    "\n",
    "### $(g_1)$ direction\n",
    "\n",
    "$$\n",
    "\\widehat d_{g_1,b}\n",
    "= \\frac{1}{n}\\sum_{i=1}^n h_b(X_i)\n",
    "\\Big(1 - \\frac{D_i}{m_i^\\tau}\\Big),\n",
    "\\qquad\n",
    "\\mathrm{se}(\\widehat d_{g_1,b})\n",
    "= \\frac{\\operatorname{sd}\\!\\left[h_b(X_i)\n",
    "\\left(1-\\frac{D_i}{m_i^\\tau}\\right)\\right]}{\\sqrt{n}}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "t_{g_1,b} = \\frac{\\widehat d_{g_1,b}}{\\mathrm{se}(\\widehat d_{g_1,b})},\n",
    "\\qquad\n",
    "\\boxed{\n",
    "\\max_{|t|_{g_1}} = \\max_b |t_{g_1,b}|\n",
    "}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### $(g_0)$ direction\n",
    "\n",
    "$$\n",
    "\\widehat d_{g_0,b}\n",
    "= \\frac{1}{n}\\sum_{i=1}^n h_b(X_i)\n",
    "\\Big(\\frac{1-D_i}{1-m_i^\\tau} - 1\\Big),\n",
    "\\qquad\n",
    "\\mathrm{se}(\\widehat d_{g_0,b})\n",
    "= \\frac{\\operatorname{sd}\\!\\left[h_b(X_i)\n",
    "\\left(\\frac{1-D_i}{1-m_i^\\tau} - 1\\right)\\right]}{\\sqrt{n}}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "t_{g_0,b} = \\frac{\\widehat d_{g_0,b}}{\\mathrm{se}(\\widehat d_{g_0,b})},\n",
    "\\qquad\n",
    "\\boxed{\n",
    "\\max_{|t|_{g_0}} = \\max_b |t_{g_0,b}|\n",
    "}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### $(m)$ direction\n",
    "\n",
    "$$\n",
    "S_i \\equiv\n",
    "\\frac{D_i(Y_i - g_{1,i})}{(m_i^\\tau)^2}\n",
    "+ \\frac{(1-D_i)(Y_i - g_{0,i})}{(1 - m_i^\\tau)^2}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\widehat d_{m,b}\n",
    "= -\\frac{1}{n}\\sum_{i=1}^n h_b(X_i) S_i,\n",
    "\\qquad\n",
    "\\mathrm{se}(\\widehat d_{m,b})\n",
    "= \\frac{\\operatorname{sd}\\!\\left[h_b(X_i)S_i\\right]}{\\sqrt{n}}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "t_{m,b} = \\frac{\\widehat d_{m,b}}{\\mathrm{se}(\\widehat d_{m,b})},\n",
    "\\qquad\n",
    "\\boxed{\n",
    "\\max_{|t|_{m}} = \\max_b |t_{m,b}|\n",
    "}.\n",
    "$$\n",
    "\n",
    "**Interpretation:** under Neyman orthogonality, each derivative mean $(\\widehat d_{\\bullet,b})$ should be approximately zero, so all $(|t_{\\bullet,b}|)$ should be small. Large $(\\max_{|t|})$ values flag miscalibration of the corresponding nuisance.\n",
    "\n",
    "---\n",
    "\n",
    "### ATTE / ATT case\n",
    "\n",
    "Let $(p_1 = \\mathbb{E}[D])$ and define the odds $(o_i = m_i^\\tau / (1 - m_i^\\tau))$.\n",
    "\n",
    "* The $(g_1)$ derivative is identically zero:\n",
    "\n",
    "  $$\n",
    "  \\Rightarrow\\quad \\max_{|t|_{g_1}} = 0.\n",
    "  $$\n",
    "\n",
    "* **$(g_0)$ direction**\n",
    "\n",
    "  $$\n",
    "  \\widehat d_{g_0,b}\n",
    "  = \\frac{1}{n}\\sum_i h_b(X_i)\\frac{(1-D_i)o_i - D_i}{p_1},\n",
    "  \\qquad\n",
    "  t_{g_0,b}\n",
    "  = \\frac{\\widehat d_{g_0,b}}{\\mathrm{se}(\\widehat d_{g_0,b})},\n",
    "  \\qquad\n",
    "  \\max_{|t|_{g_0}} = \\max_b |t_{g_0,b}|.\n",
    "  $$\n",
    "\n",
    "* **$(m)$ direction**\n",
    "\n",
    "  $$\n",
    "  \\widehat d_{m,b}\n",
    "  = -\\frac{1}{n}\\sum_i h_b(X_i)\n",
    "  \\frac{(1-D_i)(Y_i - g_{0,i})}\n",
    "       {p_1(1 - m_i^\\tau)^2},\n",
    "  \\qquad\n",
    "  \\max_{|t|_{m}} = \\max_b |t_{m,b}|.\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "**Rule of thumb:**\n",
    "$(\\max_{|t|} \\lesssim 2)$ is “okay”; larger values indicate orthogonality breakdown — fix by recalibrating that nuisance, changing learners, features, or trimming.\n"
   ],
   "id": "e8e39729c4c4c10c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:48:05.928950Z",
     "start_time": "2025-10-21T11:48:05.924729Z"
    }
   },
   "cell_type": "code",
   "source": "rep_score['orthogonality_derivatives']",
   "id": "89fc102806dbe24c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   basis      d_g1     se_g1      t_g1      d_g0     se_g0      t_g0  \\\n",
       "0      0 -0.233097  0.053585 -4.350018  0.036084  0.017458  2.066835   \n",
       "1      1 -0.012467  0.058847 -0.211863  0.029152  0.025305  1.152026   \n",
       "2      2  0.021350  0.060963  0.350206  0.038320  0.022736  1.685394   \n",
       "3      3  0.125716  0.061772  2.035176  0.047856  0.023043  2.076780   \n",
       "4      4  0.007767  0.047830  0.162379  0.052762  0.029293  1.801146   \n",
       "5      5  0.007035  0.054763  0.128462  0.002395  0.015985  0.149811   \n",
       "\n",
       "        d_m      se_m       t_m  \n",
       "0  0.314777  3.739844  0.084168  \n",
       "1  0.598770  3.992048  0.149991  \n",
       "2 -5.950311  5.773734 -1.030583  \n",
       "3  2.428545  5.321692  0.456348  \n",
       "4 -1.800507  2.686426 -0.670224  \n",
       "5  2.012890  3.491102  0.576577  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>basis</th>\n",
       "      <th>d_g1</th>\n",
       "      <th>se_g1</th>\n",
       "      <th>t_g1</th>\n",
       "      <th>d_g0</th>\n",
       "      <th>se_g0</th>\n",
       "      <th>t_g0</th>\n",
       "      <th>d_m</th>\n",
       "      <th>se_m</th>\n",
       "      <th>t_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.233097</td>\n",
       "      <td>0.053585</td>\n",
       "      <td>-4.350018</td>\n",
       "      <td>0.036084</td>\n",
       "      <td>0.017458</td>\n",
       "      <td>2.066835</td>\n",
       "      <td>0.314777</td>\n",
       "      <td>3.739844</td>\n",
       "      <td>0.084168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.012467</td>\n",
       "      <td>0.058847</td>\n",
       "      <td>-0.211863</td>\n",
       "      <td>0.029152</td>\n",
       "      <td>0.025305</td>\n",
       "      <td>1.152026</td>\n",
       "      <td>0.598770</td>\n",
       "      <td>3.992048</td>\n",
       "      <td>0.149991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.021350</td>\n",
       "      <td>0.060963</td>\n",
       "      <td>0.350206</td>\n",
       "      <td>0.038320</td>\n",
       "      <td>0.022736</td>\n",
       "      <td>1.685394</td>\n",
       "      <td>-5.950311</td>\n",
       "      <td>5.773734</td>\n",
       "      <td>-1.030583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.125716</td>\n",
       "      <td>0.061772</td>\n",
       "      <td>2.035176</td>\n",
       "      <td>0.047856</td>\n",
       "      <td>0.023043</td>\n",
       "      <td>2.076780</td>\n",
       "      <td>2.428545</td>\n",
       "      <td>5.321692</td>\n",
       "      <td>0.456348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.007767</td>\n",
       "      <td>0.047830</td>\n",
       "      <td>0.162379</td>\n",
       "      <td>0.052762</td>\n",
       "      <td>0.029293</td>\n",
       "      <td>1.801146</td>\n",
       "      <td>-1.800507</td>\n",
       "      <td>2.686426</td>\n",
       "      <td>-0.670224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.007035</td>\n",
       "      <td>0.054763</td>\n",
       "      <td>0.128462</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.015985</td>\n",
       "      <td>0.149811</td>\n",
       "      <td>2.012890</td>\n",
       "      <td>3.491102</td>\n",
       "      <td>0.576577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## `oos_tstat_fold, oos_tstat_strict`",
   "id": "9f459af6b15762d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here’s the math behind the two OOS (out-of-sample) moment t-stats used in the diagnostics.\n",
    "Assume K-fold cross-fitting with held-out index sets $(I_k)$ (size $n_k$) and complements $(R_k)$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1 — Leave-fold-out $(\\hat\\theta_{-k})$**\n",
    "\n",
    "For the moment condition\n",
    "$(\\mathbb{E}[\\psi_a(W)\\,\\theta + \\psi_b(W)] = 0)$,\n",
    "the leave-fold-out estimate used on fold $(k)$ is\n",
    "\n",
    "$$\n",
    "\\hat\\theta_{-k}\n",
    "= -\\frac{\\bar\\psi_{b,R_k}}{\\bar\\psi_{a,R_k}},\n",
    "\\qquad\n",
    "\\bar\\psi_{\\cdot,R_k}\n",
    "= \\frac{1}{|R_k|}\\sum_{i\\in R_k}\\psi_{\\cdot}(W_i).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2 — Held-out scores on fold $(k)$**\n",
    "\n",
    "Define the fold-specific held-out score for $i\\in I_k$:\n",
    "\n",
    "$$\n",
    "\\psi_i^{(k)}\n",
    "= \\psi_b(W_i) + \\psi_a(W_i)\\,\\hat\\theta_{-k}.\n",
    "$$\n",
    "\n",
    "Compute per-fold mean and variance:\n",
    "\n",
    "$$\n",
    "\\bar\\psi_k\n",
    "= \\frac{1}{n_k}\\sum_{i\\in I_k}\\psi_i^{(k)},\n",
    "\\qquad\n",
    "s_k^2\n",
    "= \\frac{1}{n_k-1}\\sum_{i\\in I_k}\\!\\big(\\psi_i^{(k)}-\\bar\\psi_k\\big)^2.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### OOS t-stat diagnostics\n",
    "\n",
    "---\n",
    "\n",
    "### **$(\\texttt{oos\\_tstat\\_fold})$**\n",
    "\n",
    "A fold-aggregated, variance-weighted t-statistic:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\texttt{oos\\_tstat\\_fold}\n",
    "= \\frac{\\displaystyle \\sum_{k=1}^K n_k\\,\\bar\\psi_k}\n",
    "       {\\displaystyle \\sqrt{\\sum_{k=1}^K n_k\\,s_k^2}}\n",
    "}\n",
    "$$\n",
    "\n",
    "*Intuition:* averages fold means and scales by a fold-pooled standard error.\n",
    "\n",
    "---\n",
    "\n",
    "### **$(\\texttt{oos\\_tstat\\_strict})$**\n",
    "\n",
    "A “strict” t-stat using every held-out observation directly:\n",
    "\n",
    "$$\n",
    "N = \\sum_{k=1}^K n_k,\n",
    "\\qquad\n",
    "\\bar\\psi_{\\text{all}}\n",
    "= \\frac{1}{N}\\sum_{k=1}^K\\sum_{i\\in I_k}\\psi_i^{(k)}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "s_{\\text{all}}^2\n",
    "= \\frac{1}{N-1}\n",
    "  \\sum_{k=1}^K\\sum_{i\\in I_k}\n",
    "  \\big(\\psi_i^{(k)} - \\bar\\psi_{\\text{all}}\\big)^2.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\texttt{oos\\_tstat\\_strict}\n",
    "= \\frac{\\bar\\psi_{\\text{all}}}{s_{\\text{all}}/\\sqrt{N}}\n",
    "}\n",
    "$$\n",
    "\n",
    "*Intuition:* computes a single overall mean and standard error across *all* held-out scores\n",
    "(often slightly more conservative).\n",
    "\n",
    "---\n",
    "\n",
    "### **Interpretation**\n",
    "\n",
    "Under a valid design and correct cross-fitting\n",
    "(so that $\\mathbb{E}[\\psi]=0$ out-of-sample), both statistics are approximately standard normal:\n",
    "\n",
    "$$\n",
    "\\text{two-sided p-value}\n",
    "\\approx 2\\big(1 - \\Phi(|t|)\\big).\n",
    "$$\n",
    "\n",
    "Values near $0$ indicate that the moment condition holds out of sample.\n",
    "Large $|t|$ suggests overfitting, leakage, or nuisance miscalibration.\n"
   ],
   "id": "ddd851912dfcaa12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:48:05.955397Z",
     "start_time": "2025-10-21T11:48:05.951926Z"
    }
   },
   "cell_type": "code",
   "source": "rep_score['oos_moment_test']",
   "id": "4188e9164590e163",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fold_results':    fold     n  psi_mean    psi_var\n",
       " 0     0  2500 -0.002503  37.561660\n",
       " 1     1  2500  0.100558  54.360122\n",
       " 2     2  2500  0.068724  31.028728\n",
       " 3     3  2500 -0.166779  32.522161,\n",
       " 'tstat_fold_agg': -2.5529434141490394e-15,\n",
       " 'pvalue_fold_agg': 0.999999999999998,\n",
       " 'tstat_strict': -2.461798420221801e-15,\n",
       " 'pvalue_strict': 0.999999999999998,\n",
       " 'interpretation': 'Near 0 indicates moment condition holds.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SUTVA",
   "id": "d6105fbdb2e03d8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:48:05.994072Z",
     "start_time": "2025-10-21T11:48:05.992235Z"
    }
   },
   "cell_type": "code",
   "source": "print_sutva_questions()",
   "id": "5da9eb09d98174df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.) Are your clients independent (i)?\n",
      "2.) Do you measure cofounders, treatment, and outcome in the same intervals?\n",
      "3.) Do you measure cofounders before treatment and outcome after?\n",
      "4.) Do you have a consistent label of treatment, such as if a person does not receive a treatment, he has a label 0?\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Those assumptions are statistically untestable. We need design of research for them",
   "id": "a77a561d0d8ead05"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Uncofoundedness",
   "id": "6cc5a6c4310f3c68"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:48:06.007556Z",
     "start_time": "2025-10-21T11:48:06.001428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from causalis.scenarios.unconfoundedness.refutation.uncofoundedness.uncofoundedness_validation import run_uncofoundedness_diagnostics\n",
    "\n",
    "rep_uc = run_uncofoundedness_diagnostics(res=ate_result)\n",
    "rep_uc['summary']"
   ],
   "id": "c9e093d82d75a162",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                    metric     value    flag\n",
       "0          balance_max_smd  0.144968  YELLOW\n",
       "1  balance_frac_violations  0.200000  YELLOW"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>balance_max_smd</td>\n",
       "      <td>0.144968</td>\n",
       "      <td>YELLOW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>balance_frac_violations</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>YELLOW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## `balance\\_max\\_smd`",
   "id": "646366d3cf870f53"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "For each covariate $(X_j)$, the (weighted) standardized mean difference is\n",
    "\n",
    "$$\n",
    "\\mathrm{SMD}_j\n",
    "= \\frac{\\big|\\mu_{1j} - \\mu_{0j}\\big|}\n",
    "       {\\sqrt{\\tfrac{1}{2}\\big(\\sigma_{1j}^2 + \\sigma_{0j}^2\\big)}}.\n",
    "$$\n",
    "\n",
    "Group means and variances are computed under the IPW weights implied by your estimand:\n",
    "\n",
    "* **ATE:** $w_{1i} = \\tfrac{D_i}{\\hat m_i}$, $w_{0i} = \\tfrac{1-D_i}{1-\\hat m_i}$\n",
    "* **ATTE:** $w_{1i} = D_i$, $w_{0i} = (1-D_i)\\tfrac{\\hat m_i}{1-\\hat m_i}$\n",
    "\n",
    "(If `normalize=True`, each weight vector is divided by its mean.)\n",
    "\n",
    "Weighted means and variances:\n",
    "\n",
    "$$\n",
    "\\mu_{gj} = \\frac{\\sum_i w_{gi} X_{ij}}{\\sum_i w_{gi}},\n",
    "\\qquad\n",
    "\\sigma_{gj}^2 = \\frac{\\sum_i w_{gi}(X_{ij} - \\mu_{gj})^2}{\\sum_i w_{gi}},\n",
    "\\qquad\n",
    "g \\in \\{0,1\\}.\n",
    "$$\n",
    "\n",
    "Special cases in the code:\n",
    "\n",
    "* If both variances are $\\approx 0$ and $|\\mu_{1j}-\\mu_{0j}| \\approx 0$ ⇒ $\\mathrm{SMD}_j = 0$\n",
    "* If both variances are $\\approx 0$ but means differ ⇒ $\\mathrm{SMD}_j = \\infty$\n",
    "* If denominator is $\\approx 0$ otherwise ⇒ $\\mathrm{SMD}_j = \\text{NaN}$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\textbf{balance\\_max\\_smd}\n",
    "= \\max_j \\mathrm{SMD}_j,\n",
    "$$\n",
    "\n",
    "implemented as a `nanmax` over the vector of $\\mathrm{SMD}_j$.\n",
    "NaNs are ignored; if any feature produced $\\infty$, the max is $\\infty$."
   ],
   "id": "1048bfc310464930"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## `balance\\_frac\\_violations`\n",
    "\n",
    "Let the SMD threshold be $\\tau$ (default $0.10$).\n",
    "Define the set of **finite** SMDs:\n",
    "\n",
    "$$\n",
    "\\mathcal{J} = \\{ j : \\ \\mathrm{SMD}_j \\text{ is finite} \\}.\n",
    "$$\n",
    "\n",
    "Then the fraction of violations is\n",
    "\n",
    "$$\n",
    "\\textbf{balance\\_frac\\_violations}\n",
    "= \\frac{1}{|\\mathcal{J}|}\n",
    "  \\sum_{j \\in \\mathcal{J}} \\mathbf{1}\\{ \\mathrm{SMD}_j \\ge \\tau \\}.\n",
    "$$\n",
    "\n",
    "So it’s the share of covariates whose **weighted** SMD exceeds the threshold,\n",
    "computed only over finite SMDs (NaN / Inf are excluded from the denominator).\n",
    "\n",
    "---\n",
    "\n",
    "### Quick interpretation\n",
    "\n",
    "* Smaller is better. A common rule of thumb is $\\mathrm{SMD} \\le 0.10$.\n",
    "* `balance_max_smd` tells you the **worst** residual imbalance across covariates.\n",
    "* `balance_frac_violations` tells you **how many** covariates (as a fraction) still exceed the chosen threshold."
   ],
   "id": "b090f89b63a1be0e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## `Sensitivity analysis`",
   "id": "f5f5270bbc445898"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1) `sensitivity_analysis`: bias-aware CI\n",
    "\n",
    "**Goal.**\n",
    "Start from your estimator $(\\hat\\theta)$ with sampling standard error $(se)$.\n",
    "Allow a controlled amount of *worst-case hidden cofounding* through three knobs $(cf_y, cf_d, \\rho)$.\n",
    "Inflate the uncertainty by an additive “max bias”.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step A — Sampling part**\n",
    "\n",
    "* Point estimate $\\hat\\theta$, standard error $se$, and $z_\\alpha$ for level $(1-\\alpha)$.\n",
    "* Usual sampling CI:\n",
    "*\n",
    "  $$\n",
    "  [\\,\\hat\\theta - z_\\alpha\\,se,\\ \\hat\\theta + z_\\alpha\\,se\\,].\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### **Step B — Cofounding geometry**\n",
    "\n",
    "The code pulls sensitivity elements from the fitted IRM:\n",
    "\n",
    "* $\\sigma^2$: the **asymptotic variance** of the estimator’s EIF\n",
    "  (so that $se = \\sqrt{\\sigma^2}$ in the module’s normalization).\n",
    "\n",
    "* $m_\\alpha(i) \\ge 0$: per-unit weight for the **outcome channel**\n",
    "  (how outcome-model misspecification moves the EIF).\n",
    "\n",
    "* $r(i)$ (“riesz\\_rep”): per-unit weight for the **treatment channel**\n",
    "  (how propensity-model misspecification moves the EIF).\n",
    "\n",
    "We turn the user’s sensitivity knobs into a **quadratic budget** for adversarial cofounding:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a_i &:= \\sqrt{2\\,m_\\alpha(i)}, \\\\[1mm]\n",
    "b_i &:=\n",
    "\\begin{cases}\n",
    "|r(i)|, & \\text{(default, worst-case sign)} \\\\\n",
    "r(i),   & \\text{(if \\texttt{use\\_signed\\_rr=True})}\n",
    "\\end{cases} \\\\[2mm]\n",
    "\\text{base}_i &= a_i^2\\,cf_y + b_i^2\\,cf_d\n",
    "  + 2\\,\\rho\\,\\sqrt{cf_y\\,cf_d}\\,a_i b_i \\ge 0, \\\\[2mm]\n",
    "\\nu^2 &:= \\mathbb{E}_n[\\text{base}_i].\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "- $cf_y \\ge 0$: strength of unobserved outcome disturbance\n",
    "- $cf_d \\ge 0$: strength of unobserved treatment disturbance\n",
    "- $\\rho \\in [-1,1]$: their correlation\n",
    "\n",
    "This $\\nu^2$ is a **dimensionless bias multiplier** — how sensitive the EIF is to those perturbations.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step C — Max bias and intervals**\n",
    "\n",
    "Two equivalent forms appear in the code:\n",
    "\n",
    "$$\n",
    "\\text{max\\_bias}\n",
    "= \\sqrt{\\sigma^2\\,\\nu^2}\n",
    "= \\big(\\sqrt{\\nu^2}\\big)\\,se.\n",
    "$$\n",
    "\n",
    "Then the module reports:\n",
    "\n",
    "* **Cofounding bounds for $\\theta$:**\n",
    "\n",
    "  $$\n",
    "  [\\,\\hat\\theta - \\text{max\\_bias},\\; \\hat\\theta + \\text{max\\_bias}\\,].\n",
    "  $$\n",
    "\n",
    "* **Bias-aware CI (sampling + cofounding, worst-case additive):**\n",
    "\n",
    "  $$\n",
    "  \\Big[\\,\\hat\\theta - (\\text{max\\_bias} + z_\\alpha\\,se),\\;\n",
    "  \\hat\\theta + (\\text{max\\_bias} + z_\\alpha\\,se)\\,\\Big].\n",
    "  $$\n",
    "\n",
    "(So you’re adding sampling error and the adversarial bias *linearly* for a conservative envelope.)\n",
    "\n",
    "---\n",
    "\n",
    "**Notes & edge handling**\n",
    "\n",
    "* Numeric PSD clamping ensures $\\text{base}_i \\ge 0$; $\\rho$ is clipped to $[-1,1]$.\n",
    "* If $cf_y = cf_d = 0 \\Rightarrow \\nu^2 = 0 \\Rightarrow$ bias-aware CI collapses to the sampling CI.\n",
    "* Internally, a delta-method IF for $\\text{max\\_bias}$ is\n",
    "\n",
    "  $$\n",
    "  \\psi_{\\text{max}}(i)\n",
    "  = \\frac{\\sigma^2\\,\\psi_{\\nu^2}(i) + \\nu^2\\,\\psi_{\\sigma^2}(i)}\n",
    "         {2\\,\\text{max\\_bias}},\n",
    "  $$\n",
    "  matching $\\text{max\\_bias} = \\sqrt{\\sigma^2\\nu^2}$ (used for coherent summaries).\n",
    "\n",
    "---\n",
    "\n",
    "### 2) `sensitivity_benchmark`: calibrating $(cf_y, cf_d, \\rho)$ from omitted covariates\n",
    "\n",
    "**Goal.**\n",
    "Pick a set $Z$ of candidate “omitted” covariates (the `benchmarking_set`).\n",
    "Refit a **short** IRM that excludes $Z$ and compare it to the **long** (original) model.\n",
    "Use how well $Z$ explains residual variation to *derive* plausible $(cf_y, cf_d, \\rho)$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step A — Long vs short estimates**\n",
    "\n",
    "* Long: $\\hat\\theta_{\\text{long}}$ (original model).\n",
    "* Short: $\\hat\\theta_{\\text{short}}$ (drop $Z$, same learners/hyperparams).\n",
    "* Report $\\Delta = \\hat\\theta_{\\text{long}} - \\hat\\theta_{\\text{short}}$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step B — Residuals from the long model**\n",
    "\n",
    "Let $(g_1, g_0, \\hat m)$ be the outcome and propensity learners:\n",
    "\n",
    "$$\n",
    "r_y := Y - \\big(D g_1 + (1-D) g_0\\big),\n",
    "\\qquad\n",
    "r_d := D - \\hat m.\n",
    "$$\n",
    "\n",
    "These are the EIF’s outcome and treatment residual components.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step C — How much of each residual does $Z$ explain?**\n",
    "\n",
    "Regress $r_y$ on $Z$ and $r_d$ on $Z$ (unweighted OLS; **ATT** case uses ATT weights):\n",
    "\n",
    "* Obtain $R^2_y$ and $R^2_d$.\n",
    "* Convert to **signal-to-noise ratios** (the “strength” of cofounding channels):\n",
    "\n",
    "  $$\n",
    "  cf_y = \\frac{R^2_y}{1 - R^2_y},\n",
    "  \\qquad\n",
    "  cf_d = \\frac{R^2_d}{1 - R^2_d}.\n",
    "  $$\n",
    "\n",
    "  (These are the same $R^2 / (1 - R^2)$ maps used in modern partial-$R^2$ robustness frameworks.)\n",
    "\n",
    "Compute the correlation between the **fitted** pieces from those two regressions:\n",
    "\n",
    "$$\n",
    "\\rho = \\operatorname{corr}\\!\\big(\\widehat r_y(Z),\\ \\widehat r_d(Z)\\big),\n",
    "$$\n",
    "\n",
    "weighted for ATT when applicable, then clipped to $[-1,1]$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Outputs**\n",
    "\n",
    "A one-row DataFrame (indexed by the treatment name) with\n",
    "\n",
    "$$\n",
    "\\{\\, cf_y,\\ cf_d,\\ \\rho,\\ \\hat\\theta_{\\text{long}},\\\n",
    "\\hat\\theta_{\\text{short}},\\ \\Delta \\,\\}.\n",
    "$$\n",
    "\n",
    "You can pass $(cf_y, cf_d, \\rho)$ straight into `sensitivity_analysis` to get the associated bias-aware interval.\n",
    "Intuitively, this calibrates *how strong hidden stuff would need to be* by using a concrete, observed proxy $Z$.\n",
    "\n",
    "---\n",
    "\n",
    "## **How to read them together**\n",
    "\n",
    "1. Use `sensitivity_benchmark` with a plausible omitted set $Z$ to **derive** $(cf_y, cf_d, \\rho)$ and observe the actual estimate shift $\\Delta$.\n",
    "\n",
    "2. Plug those $(cf_y, cf_d, \\rho)$ into `sensitivity_analysis` to get:\n",
    "\n",
    "   $$\n",
    "   \\text{max\\_bias} = \\sqrt{\\nu^2}\\,se,\n",
    "   \\qquad\n",
    "   \\text{Bias-aware CI} = \\hat\\theta \\pm (\\text{max\\_bias} + z_\\alpha\\,se).\n",
    "   $$\n",
    "\n",
    "Small $cf$ values (or $\\rho \\approx 0$) ⇒ tiny $\\nu^2$ ⇒ bias-aware CI near the sampling CI.\n",
    "Large $cf$ values and $|\\rho|\\approx 1$ widen it, reflecting stronger plausible hidden cofounding.\n"
   ],
   "id": "aa4d4d1ecd441866"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:48:06.035999Z",
     "start_time": "2025-10-21T11:48:06.033423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from causalis.scenarios.unconfoundedness.refutation.uncofoundedness.sensitivity import (\n",
    "    sensitivity_analysis, sensitivity_benchmark\n",
    ")\n",
    "\n",
    "sensitivity_analysis(ate_result, cf_y=0.01, cf_d=0.01, rho=1.0, level=0.95)"
   ],
   "id": "b85b44ef42ad3ebd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'theta': 0.9917276396749556,\n",
       " 'se': 0.06233979878689177,\n",
       " 'level': 0.95,\n",
       " 'z': 1.959963984540054,\n",
       " 'sampling_ci': (0.869543879249174, 1.1139114001007373),\n",
       " 'theta_bounds_cofounding': (0.9282979061474285, 1.0551573732024828),\n",
       " 'bias_aware_ci': (0.8061141457216469, 1.1773411336282644),\n",
       " 'max_bias': 0.06342973352752714,\n",
       " 'sigma2': 1.0690078122954034,\n",
       " 'nu2': 1.0352732234146504,\n",
       " 'params': {'cf_y': 0.01, 'cf_d': 0.01, 'rho': 1.0, 'use_signed_rr': False}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T11:48:13.714650Z",
     "start_time": "2025-10-21T11:48:06.062578Z"
    }
   },
   "cell_type": "code",
   "source": "sensitivity_benchmark(ate_result, benchmarking_set =['tenure_months'])",
   "id": "e95e2fc197e48bba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       cf_y          cf_d  rho  theta_long  theta_short    delta\n",
       "d  0.000001  1.951733e-08 -1.0    0.991728     1.064098 -0.07237"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cf_y</th>\n",
       "      <th>cf_d</th>\n",
       "      <th>rho</th>\n",
       "      <th>theta_long</th>\n",
       "      <th>theta_short</th>\n",
       "      <th>delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.951733e-08</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.991728</td>\n",
       "      <td>1.064098</td>\n",
       "      <td>-0.07237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
